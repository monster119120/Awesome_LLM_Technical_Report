# Awesome_LLM_Technical_Report

inclusionAI
- [MoE scaling law](https://arxiv.org/pdf/2507.17702)
- [Decay free pretrain](https://arxiv.org/pdf/2507.17634)
- [Mid-train data mix](https://arxiv.org/pdf/2503.05139)
- [Ling V1](https://arxiv.org/pdf/2503.05139)
- [Ling V2](https://arxiv.org/pdf/2510.22115)
- [Ling V2.5](https://huggingface.co/inclusionAI/Ring-2.5-1T)       混合线性注意力

OpenAI
- [GPT-OSS](https://arxiv.org/pdf/2508.10925)   混合线性注意力

Xiaomi
- [HySparse](https://arxiv.org/pdf/2602.03560)  
- [MiMo-V2-Flash](https://arxiv.org/pdf/2601.02780)     混合线性注意力

Meituan
- [LongCat-Flash-Thinking-2601](https://arxiv.org/pdf/2601.16725)   混合线性注意力
- [ZigZag Attention](https://arxiv.org/pdf/2512.23966)
- [LongCat-Flash-Thinking](https://arxiv.org/pdf/2509.18883)
- [LongCat-Flash](https://arxiv.org/pdf/2509.01322)
- [N-gram](https://arxiv.org/pdf/2601.21204)

Xiaohongshu
- [Dots.llm](https://arxiv.org/pdf/2506.05767)

Google
- [Gemma3](https://arxiv.org/pdf/2503.19786)
- [Gemma2](https://arxiv.org/pdf/2408.00118)
- [Gemma](https://arxiv.org/pdf/2403.08295)

Qwen
- [Qwen3](https://arxiv.org/pdf/2505.09388)
- [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)
- [Qwen2.5](https://arxiv.org/pdf/2412.15115)
- [Qwen2.5-1M](https://arxiv.org/pdf/2501.15383)

Stepfun-ai
- [Step-3.5-Flash](https://arxiv.org/pdf/2602.10604)    混合线性注意力
- [Step-3](https://arxiv.org/pdf/2507.19427)

DeepSeek ([HuggingFace](https://huggingface.co/deepseek-ai))
- [N-gram](https://arxiv.org/pdf/2601.07372)
- [mHC](https://arxiv.org/pdf/2512.24880)
- [NSA](https://arxiv.org/pdf/2502.11089)
- [DeepSeekV3](https://arxiv.org/pdf/2412.19437)
- [DeepSeekV3-Infra](https://arxiv.org/pdf/2505.09343) 
- [DeepSeek-V2](https://arxiv.org/pdf/2405.04434)

Kimi
- [Kimi-K2.5](https://huggingface.co/moonshotai/Kimi-K2.5)  全注意力
- [Kimi-Linear](https://www.arxiv.org/pdf/2510.26692)
- [MoBA](https://arxiv.org/pdf/2502.13189)
- [Kimi-K2](https://arxiv.org/pdf/2507.20534)
- [Kimi-K1.5](https://arxiv.org/pdf/2501.12599)
- [Kimi-K1](https://arxiv.org/pdf/2502.16982)

Minimax ([HuggingFace](https://huggingface.co/MiniMaxAI))
- [Minimax-M2.5](https://huggingface.co/MiniMaxAI/MiniMax-M2.5)     全注意力
- [Minimax-M2.1](https://www.minimax.io/news/minimax-m21)
- [Minimax-M2](https://www.minimax.io/news/minimax-m2) | [HuggingFace](https://huggingface.co/MiniMaxAI/MiniMax-M2)
- [Minimax-M1](https://www.arxiv.org/pdf/2506.13585) | [HuggingFace](https://huggingface.co/MiniMaxAI/MiniMax-M1-80k)

ZhipuAI ([HuggingFace](https://huggingface.co/zai-org))
- [GLM-5](https://z.ai/blog/glm-5)  混合线性注意力
- [GLM-4.7](https://z.ai/blog/glm-4.7) | [HuggingFace](https://huggingface.co/zai-org/GLM-4.7)
- [GLM-4.6](https://z.ai/blog/glm-4.6) | [HuggingFace](https://huggingface.co/zai-org/GLM-4.6)
- [GLM-4.5](https://www.arxiv.org/pdf/2508.06471) | [HuggingFace](https://huggingface.co/zai-org/GLM-4.5)
- [GLM-4](https://www.arxiv.org/pdf/2406.12793) | [HuggingFace](https://huggingface.co/zai-org/GLM-4-32B-0414)